{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1e7Z6yANh7geIqyTa6zNYP2g8X9XhUqAY","timestamp":1758373647368}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Segmentation de dÃ©chets avec YOLOv8 - TEKBOT Robotics Challenge 2025\n","La segmentation des instances va plus loin que la dÃ©tection des objets et consiste Ã  identifier des objets individuels dans une image et Ã  les segmenter par rapport au reste de l'image.\n","\n","Le rÃ©sultat d'un modÃ¨le de segmentation d'instances est un ensemble de masques ou de contours qui dÃ©limitent chaque objet de l'image, ainsi que des Ã©tiquettes de classe et des scores de confiance pour chaque objet. La segmentation d'instances est utile lorsque vous avez besoin de savoir non seulement oÃ¹ se trouvent les objets dans une image, mais aussi quelle est leur forme exacte."],"metadata":{"id":"qDtp_XFlpgII"}},{"cell_type":"markdown","source":["## Configuration et importation des bibliothÃ¨ques"],"metadata":{"id":"D0bse4Q5qkkC"}},{"cell_type":"code","source":["!pip install ultralytics roboflow -q"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"vIOO2Ljkhbcy","outputId":"a776eb83-8c8e-4d5f-8918-b09f76467594"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/1.1 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[90mâ•º\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.6/1.1 MB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/88.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m88.7/88.7 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m66.8/66.8 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m49.9/49.9 MB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m60.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m4.2/4.2 MB\u001b[0m \u001b[31m88.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}]},{"cell_type":"markdown","source":["---\n","\n","## ðŸ—‚ï¸ Collecte et annotation des donnÃ©es\n","\n","Pour entraÃ®ner un modÃ¨le de segmentation des instances sur des **images de dÃ©chets**, vous devez constituer un **jeu de donnÃ©es structurÃ©** et bien annotÃ©.\n","\n","Commencez par rÃ©unir un ensemble reprÃ©sentatif dâ€™**images de dÃ©chets** (photos prises sur le terrain, tÃ©lÃ©chargÃ©es ou issues de bases de donnÃ©es ouvertes). Chaque image devra ensuite Ãªtre annotÃ©e avec des **masques polygonaux** reprÃ©sentant chaque type de dÃ©chet de maniÃ¨re individuelle.\n","\n","Le jeu de donnÃ©es final doit respecter la structure suivante :\n","\n","```\n","dataset/\n","â”œâ”€â”€ images/\n","â”‚   â”œâ”€â”€ train/    # Images utilisÃ©es pour l'entraÃ®nement\n","â”‚   â””â”€â”€ val/      # Images utilisÃ©es pour la validation\n","â”œâ”€â”€ labels/\n","â”‚   â”œâ”€â”€ train/    # Fichiers dâ€™annotation des images dâ€™entraÃ®nement (format YOLO + segmentation)\n","â”‚   â””â”€â”€ val/      # Fichiers dâ€™annotation des images de validation\n","â”œâ”€â”€ data.yaml     # Fichier de configuration dÃ©crivant le dataset et les classes\n","```\n","\n","Le fichier `data.yaml` doit contenir :\n","\n","```yaml\n","path: dataset\n","train: images/train\n","val: images/val\n","names:\n","  0: plastique\n","  1: mÃ©tal\n","  2: papier\n","  3: verre\n","```\n","\n","ðŸ’¡ **Conseil** : veillez Ã  Ã©quilibrer les classes dans les images (ex : autant de plastique que de mÃ©tal) pour Ã©viter les biais lors de l'entraÃ®nement.\n"],"metadata":{"id":"O5jPwd5ErLso"}},{"cell_type":"markdown","source":["Voici un guide clair et structurÃ© pour l'annotation des images :\n","\n","1. **Annoter les images avec Roboflow (via navigateur)**\n","2. **Connecter ton projet Roboflow Ã  Google Colab** pour l'entraÃ®nement avec YOLOv8 (Ultralytics)\n","\n","---\n","\n","## âœ… 1. Annoter les images avec Roboflow\n","\n","### ðŸ”§ Ã‰tapes :\n","\n","1. **CrÃ©er un compte** sur [https://roboflow.com](https://roboflow.com) (gratuit).\n","2. Clique sur **\"Create New Project\"** :\n","\n","   * Nom : `dechets-segmentation`\n","   * Type de projet : `Instance Segmentation`\n","   * Annotation group : `One annotation group`\n","   * License : choisir selon ton cas\n","3. Une fois le projet crÃ©Ã©, clique sur **Upload** pour importer tes images :\n","\n","   * Format acceptÃ© : `.jpg`, `.png`, etc.\n","   * Roboflow redimensionnera les images si nÃ©cessaire.\n","4. Clique sur **Annotate** :\n","\n","   * SÃ©lectionne une image\n","   * Clique sur **\"Add Annotation\"**\n","   * Trace un **polygone** autour de chaque dÃ©chet\n","   * Donne-lui un nom (ex: `plastique`, `papier`, etc.)\n","5. Quand tu as fini, clique sur **Save**.\n","\n","---\n","\n","## ðŸ”„ 2. GÃ©nÃ©rer et exporter le dataset pour YOLOv8\n","\n","1. Une fois toutes les images annotÃ©es :\n","\n","   * Clique sur **\"Generate Dataset\"** en haut Ã  droite\n","   * Choisis :\n","\n","     * Format : **YOLOv8 Segmentation**\n","     * Taille dâ€™image : 640x640 ou selon ton besoin\n","     * Data split : train / val / test\n","     * ProcÃ©der Ã  l'augmentation Ã©ventuelle des donnÃ©es et/ou prÃ©processing\n","\n","2. Clique sur **\"Download Code\"** > **Show download code**\n","\n","   Tu obtiendras un **script d'import automatique** pour Colab, comme ceci :\n","\n","```python\n","!pip install roboflow\n","from roboflow import Roboflow\n","\n","rf = Roboflow(api_key=\"TON_API_KEY\")\n","project = rf.workspace(\"TON_WORKSPACE\").project(\"dechets-segmentation\")\n","dataset = project.version(1).download(\"yolov8\")\n","```\n","\n","---\n","\n","## ðŸ§ª 3. Lier Ã  ton notebook Google Colab\n","\n","### ðŸ“˜ Exemple complet dans Colab :\n","\n","```python\n","# 1. Installer les dÃ©pendances\n","!pip install ultralytics roboflow\n","\n","# 2. TÃ©lÃ©charger les donnÃ©es Roboflow\n","from roboflow import Roboflow\n","rf = Roboflow(api_key=\"TON_API_KEY\")\n","project = rf.workspace(\"TON_WORKSPACE\").project(\"dechets-segmentation\")\n","dataset = project.version(1).download(\"yolov8\")  # format YOLOv8 Segmentation\n","\n","# 3. EntraÃ®ner avec YOLOv8\n","from ultralytics import YOLO\n","\n","model = YOLO(\"yolov8n-seg.pt\")  # ou yolov8m-seg.pt, etc.\n","model.train(data=dataset.location + \"/data.yaml\", epochs=50, imgsz=640, task=\"segment\")\n","```\n","\n","---\n","\n","# ðŸ“„ VÃ©rifie que le fichier YAML est bien tÃ©lÃ©chargÃ©\n","**!cat {dataset.location}/data.yaml**\n","\n","## ðŸ“Œ OÃ¹ trouver les infos pour `api_key` et `workspace` ?\n","\n","* Va sur [https://app.roboflow.com](https://app.roboflow.com)\n","* Dans ton tableau de bord, clique sur ton profil â†’ **\"Your API Key\"**\n","* Le **nom du workspace** est visible dans lâ€™URL : `https://app.roboflow.com/<workspace-name>/<project-name>`\n","\n","---\n","\n"],"metadata":{"id":"ftLmeMsaqxhK"}},{"cell_type":"code","source":["# ðŸ§ª Charger ton modÃ¨le entraÃ®nÃ©\n","model = YOLO(\"yolov8-segmentation-dechets/exp/weights/best.pt\")\n","\n","# ðŸ“¸ Tester sur une image\n","results = model(\"https://images.unsplash.com/photo-1603252109303-2751441dd157\")  # image en ligne\n","results[0].show()"],"metadata":{"id":"CejI05Mtq2MQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["* ðŸ“¦ Export vers diffÃ©rents formats (`.pt`, `.onnx`, `.tflite`, `.torchscript`, etc.)\n","* ðŸ¤– DÃ©ploiement sur **Raspberry Pi**\n","* ðŸ“¡ Utilisation avec **Arduino + ESP32/ESP8266**\n","* ðŸ“± DÃ©ploiement mobile (Edge, Android, iOS)\n","\n","---\n","\n","### ðŸ”„ Ã€ ajouter aprÃ¨s l'entraÃ®nement\n","\n","```python\n","# ðŸ“¤ Exportation du modÃ¨le YOLOv8 adaptÃ© Ã  vos donÃ©es vers diffÃ©rents formats\n","\n","# ðŸ“¦ Charger ton modÃ¨le entraÃ®nÃ©\n","model = YOLO(\"yolov8-segmentation-dechets/exp/weights/best.pt\")\n","\n","# ðŸ”½ Exporter vers TorchScript (pour Raspberry Pi, C++ backend)\n","model.export(format=\"torchscript\")\n","\n","# ðŸ” Exporter vers ONNX (compatible OpenCV DNN, TensorRT, etc.)\n","model.export(format=\"onnx\")\n","\n","# ðŸ“± Exporter vers CoreML (pour iOS)\n","model.export(format=\"coreml\")\n","\n","# ðŸ¤– Exporter vers TensorFlow Lite (Edge / Android / ESP)\n","model.export(format=\"tflite\")\n","\n","# ðŸ§  Exporter vers TensorFlow SavedModel (ex : pour TensorFlow.js, etc.)\n","model.export(format=\"saved_model\")\n","\n","# ðŸ“¡ Export vers PaddlePaddle (optionnel)\n","model.export(format=\"paddle\")\n","```\n","\n","---\n","\n","### ðŸ“ Utilisation sur **Raspberry Pi** (avec TorchScript ou ONNX)\n","\n","```bash\n","# Sur Raspberry Pi (une fois le modÃ¨le exportÃ©) :\n","# Installer les dÃ©pendances\n","sudo apt install libopencv-dev python3-opencv\n","pip install torch torchvision ultralytics\n","\n","# Exemple de script Python minimal\n","import torch\n","from PIL import Image\n","import cv2\n","import numpy as np\n","\n","# Charger modÃ¨le TorchScript\n","model = torch.jit.load(\"best.torchscript\")\n","model.eval()\n","\n","# Lire une image\n","img = cv2.imread(\"image.jpg\")\n","img_resized = cv2.resize(img, (640, 640))\n","img_tensor = torch.tensor(img_resized / 255.0).permute(2, 0, 1).unsqueeze(0).float()\n","\n","# PrÃ©dire\n","with torch.no_grad():\n","    pred = model(img_tensor)\n","```\n","\n","---\n","\n","### ðŸ“¡ Utilisation avec **Arduino + ESP32/ESP8266**\n","\n","> âš ï¸ YOLOv8 ne sâ€™exÃ©cute pas directement sur un ESP32 ou un Arduino. Mais tu peux :\n","\n","* ExÃ©cuter YOLOv8 sur un **serveur (Raspberry, cloud)** et\n","* Envoyer les **rÃ©sultats via HTTP/MQTT** Ã  lâ€™ESP32/ESP8266 pour action (LED, moteur, etc.)\n","\n","#### Exemple dâ€™architecture :\n","\n","```text\n","Camera -> Raspberry Pi (YOLOv8) -> Envoi JSON vers ESP32 -> ESP rÃ©agit (alarme, LED, etc.)\n","```\n","\n","#### Exemple de code Python serveur :\n","\n","```python\n","import requests\n","import json\n","\n","results = model(\"photo.jpg\")\n","labels = results[0].boxes.cls.cpu().numpy().tolist()\n","\n","# Envoyer Ã  lâ€™ESP\n","requests.post(\"http://192.168.0.100:5000/detect\", json={\"labels\": labels})\n","```\n","\n","#### Code cÃ´tÃ© ESP (MicroPython ou Arduino C++) :\n","\n","```cpp\n","// CÃ´tÃ© Arduino ou ESP32 - lire JSON depuis HTTP/MQTT et agir\n","if (received_label == \"plastique\") {\n","  digitalWrite(LED_PIN, HIGH);\n","}\n","```\n","\n","---\n","\n","### ðŸ“± DÃ©ploiement mobile (Android/iOS avec TensorFlow Lite)\n","\n","* Utilise le fichier `best.tflite`\n","* IntÃ¨gre-le dans une app Android via **TensorFlow Lite Android API**\n","* Exemples disponibles ici : [https://www.tensorflow.org/lite/android](https://www.tensorflow.org/lite/android)\n","\n","```java\n","// Android Java\n","Interpreter tflite = new Interpreter(loadModelFile(\"best.tflite\"));\n","```\n","\n","---\n","\n","### âœ… RÃ©capitulatif des formats utiles\n","\n","| Format         | Utilisation                         |\n","| -------------- | ----------------------------------- |\n","| `.pt`          | PyTorch / Ultralytics               |\n","| `.torchscript` | Raspberry Pi / C++ / PyTorch Lite   |\n","| `.onnx`        | OpenCV DNN, TensorRT, NVIDIA Jetson |\n","| `.tflite`      | Android, Raspberry Pi, ESP-CAM      |\n","| `coreml`       | iOS (iPhone, iPad)                  |\n","| `saved_model`  | TensorFlow, TF.js, TPU              |\n","\n","---\n"],"metadata":{"id":"yspXNE3ux5Ma"}},{"cell_type":"markdown","source":["\n","\n","---\n","## Documentation complÃ©mentaire\n","\n","### Roboflow : https://www.google.com/url?sa=t&source=web&rct=j&opi=89978449&url=https://roboflow.com/&ved=2ahUKEwj4xKbqtOSNAxXCTaQEHZpOM5QQFnoECAkQAQ&usg=AOvVaw1h_eItJcl0qix45QGoUODw\n","\n","### Tensorflow : https://www.google.com/url?sa=t&source=web&rct=j&opi=89978449&url=https://www.tensorflow.org/%3Fhl%3Dfr&ved=2ahUKEwiz25OGteSNAxW1V6QEHQs2AyUQFnoECBUQAQ&usg=AOvVaw1C86YCtph6fCCJ-ya1LX3g\n","\n","### Pytorch : https://www.google.com/url?sa=t&source=web&rct=j&opi=89978449&url=https://pytorch.org/&ved=2ahUKEwiQzqOSteSNAxUXV6QEHTirDjMQFnoECBkQAQ&usg=AOvVaw2mABY6VbqZdRJYnleMzDSb\n","\n","### Ultralytics : https://www.google.com/url?sa=t&source=web&rct=j&opi=89978449&url=https://www.ultralytics.com/&ved=2ahUKEwih6cebteSNAxW8U6QEHSOwPP4QFnoECEEQAQ&usg=AOvVaw0egcjbygLKWOBkY23mDV-l\n"],"metadata":{"id":"gRZpgpP1zMj3"}},{"cell_type":"code","source":[],"metadata":{"id":"GmpfQIdxzTIH"},"execution_count":null,"outputs":[]}]}