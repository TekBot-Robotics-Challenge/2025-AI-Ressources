{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1e7Z6yANh7geIqyTa6zNYP2g8X9XhUqAY","timestamp":1758373647368}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Segmentation de déchets avec YOLOv8 - TEKBOT Robotics Challenge 2025\n","La segmentation des instances va plus loin que la détection des objets et consiste à identifier des objets individuels dans une image et à les segmenter par rapport au reste de l'image.\n","\n","Le résultat d'un modèle de segmentation d'instances est un ensemble de masques ou de contours qui délimitent chaque objet de l'image, ainsi que des étiquettes de classe et des scores de confiance pour chaque objet. La segmentation d'instances est utile lorsque vous avez besoin de savoir non seulement où se trouvent les objets dans une image, mais aussi quelle est leur forme exacte."],"metadata":{"id":"qDtp_XFlpgII"}},{"cell_type":"markdown","source":["## Configuration et importation des bibliothèques"],"metadata":{"id":"D0bse4Q5qkkC"}},{"cell_type":"code","source":["!pip install ultralytics roboflow -q"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"vIOO2Ljkhbcy","outputId":"a776eb83-8c8e-4d5f-8918-b09f76467594"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.1 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.6/1.1 MB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/88.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.7/88.7 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.8/66.8 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.9/49.9 MB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m60.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.2/4.2 MB\u001b[0m \u001b[31m88.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}]},{"cell_type":"markdown","source":["---\n","\n","## 🗂️ Collecte et annotation des données\n","\n","Pour entraîner un modèle de segmentation des instances sur des **images de déchets**, vous devez constituer un **jeu de données structuré** et bien annoté.\n","\n","Commencez par réunir un ensemble représentatif d’**images de déchets** (photos prises sur le terrain, téléchargées ou issues de bases de données ouvertes). Chaque image devra ensuite être annotée avec des **masques polygonaux** représentant chaque type de déchet de manière individuelle.\n","\n","Le jeu de données final doit respecter la structure suivante :\n","\n","```\n","dataset/\n","├── images/\n","│   ├── train/    # Images utilisées pour l'entraînement\n","│   └── val/      # Images utilisées pour la validation\n","├── labels/\n","│   ├── train/    # Fichiers d’annotation des images d’entraînement (format YOLO + segmentation)\n","│   └── val/      # Fichiers d’annotation des images de validation\n","├── data.yaml     # Fichier de configuration décrivant le dataset et les classes\n","```\n","\n","Le fichier `data.yaml` doit contenir :\n","\n","```yaml\n","path: dataset\n","train: images/train\n","val: images/val\n","names:\n","  0: plastique\n","  1: métal\n","  2: papier\n","  3: verre\n","```\n","\n","💡 **Conseil** : veillez à équilibrer les classes dans les images (ex : autant de plastique que de métal) pour éviter les biais lors de l'entraînement.\n"],"metadata":{"id":"O5jPwd5ErLso"}},{"cell_type":"markdown","source":["Voici un guide clair et structuré pour l'annotation des images :\n","\n","1. **Annoter les images avec Roboflow (via navigateur)**\n","2. **Connecter ton projet Roboflow à Google Colab** pour l'entraînement avec YOLOv8 (Ultralytics)\n","\n","---\n","\n","## ✅ 1. Annoter les images avec Roboflow\n","\n","### 🔧 Étapes :\n","\n","1. **Créer un compte** sur [https://roboflow.com](https://roboflow.com) (gratuit).\n","2. Clique sur **\"Create New Project\"** :\n","\n","   * Nom : `dechets-segmentation`\n","   * Type de projet : `Instance Segmentation`\n","   * Annotation group : `One annotation group`\n","   * License : choisir selon ton cas\n","3. Une fois le projet créé, clique sur **Upload** pour importer tes images :\n","\n","   * Format accepté : `.jpg`, `.png`, etc.\n","   * Roboflow redimensionnera les images si nécessaire.\n","4. Clique sur **Annotate** :\n","\n","   * Sélectionne une image\n","   * Clique sur **\"Add Annotation\"**\n","   * Trace un **polygone** autour de chaque déchet\n","   * Donne-lui un nom (ex: `plastique`, `papier`, etc.)\n","5. Quand tu as fini, clique sur **Save**.\n","\n","---\n","\n","## 🔄 2. Générer et exporter le dataset pour YOLOv8\n","\n","1. Une fois toutes les images annotées :\n","\n","   * Clique sur **\"Generate Dataset\"** en haut à droite\n","   * Choisis :\n","\n","     * Format : **YOLOv8 Segmentation**\n","     * Taille d’image : 640x640 ou selon ton besoin\n","     * Data split : train / val / test\n","     * Procéder à l'augmentation éventuelle des données et/ou préprocessing\n","\n","2. Clique sur **\"Download Code\"** > **Show download code**\n","\n","   Tu obtiendras un **script d'import automatique** pour Colab, comme ceci :\n","\n","```python\n","!pip install roboflow\n","from roboflow import Roboflow\n","\n","rf = Roboflow(api_key=\"TON_API_KEY\")\n","project = rf.workspace(\"TON_WORKSPACE\").project(\"dechets-segmentation\")\n","dataset = project.version(1).download(\"yolov8\")\n","```\n","\n","---\n","\n","## 🧪 3. Lier à ton notebook Google Colab\n","\n","### 📘 Exemple complet dans Colab :\n","\n","```python\n","# 1. Installer les dépendances\n","!pip install ultralytics roboflow\n","\n","# 2. Télécharger les données Roboflow\n","from roboflow import Roboflow\n","rf = Roboflow(api_key=\"TON_API_KEY\")\n","project = rf.workspace(\"TON_WORKSPACE\").project(\"dechets-segmentation\")\n","dataset = project.version(1).download(\"yolov8\")  # format YOLOv8 Segmentation\n","\n","# 3. Entraîner avec YOLOv8\n","from ultralytics import YOLO\n","\n","model = YOLO(\"yolov8n-seg.pt\")  # ou yolov8m-seg.pt, etc.\n","model.train(data=dataset.location + \"/data.yaml\", epochs=50, imgsz=640, task=\"segment\")\n","```\n","\n","---\n","\n","# 📄 Vérifie que le fichier YAML est bien téléchargé\n","**!cat {dataset.location}/data.yaml**\n","\n","## 📌 Où trouver les infos pour `api_key` et `workspace` ?\n","\n","* Va sur [https://app.roboflow.com](https://app.roboflow.com)\n","* Dans ton tableau de bord, clique sur ton profil → **\"Your API Key\"**\n","* Le **nom du workspace** est visible dans l’URL : `https://app.roboflow.com/<workspace-name>/<project-name>`\n","\n","---\n","\n"],"metadata":{"id":"ftLmeMsaqxhK"}},{"cell_type":"code","source":["# 🧪 Charger ton modèle entraîné\n","model = YOLO(\"yolov8-segmentation-dechets/exp/weights/best.pt\")\n","\n","# 📸 Tester sur une image\n","results = model(\"https://images.unsplash.com/photo-1603252109303-2751441dd157\")  # image en ligne\n","results[0].show()"],"metadata":{"id":"CejI05Mtq2MQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["* 📦 Export vers différents formats (`.pt`, `.onnx`, `.tflite`, `.torchscript`, etc.)\n","* 🤖 Déploiement sur **Raspberry Pi**\n","* 📡 Utilisation avec **Arduino + ESP32/ESP8266**\n","* 📱 Déploiement mobile (Edge, Android, iOS)\n","\n","---\n","\n","### 🔄 À ajouter après l'entraînement\n","\n","```python\n","# 📤 Exportation du modèle YOLOv8 adapté à vos donées vers différents formats\n","\n","# 📦 Charger ton modèle entraîné\n","model = YOLO(\"yolov8-segmentation-dechets/exp/weights/best.pt\")\n","\n","# 🔽 Exporter vers TorchScript (pour Raspberry Pi, C++ backend)\n","model.export(format=\"torchscript\")\n","\n","# 🔁 Exporter vers ONNX (compatible OpenCV DNN, TensorRT, etc.)\n","model.export(format=\"onnx\")\n","\n","# 📱 Exporter vers CoreML (pour iOS)\n","model.export(format=\"coreml\")\n","\n","# 🤖 Exporter vers TensorFlow Lite (Edge / Android / ESP)\n","model.export(format=\"tflite\")\n","\n","# 🧠 Exporter vers TensorFlow SavedModel (ex : pour TensorFlow.js, etc.)\n","model.export(format=\"saved_model\")\n","\n","# 📡 Export vers PaddlePaddle (optionnel)\n","model.export(format=\"paddle\")\n","```\n","\n","---\n","\n","### 🍓 Utilisation sur **Raspberry Pi** (avec TorchScript ou ONNX)\n","\n","```bash\n","# Sur Raspberry Pi (une fois le modèle exporté) :\n","# Installer les dépendances\n","sudo apt install libopencv-dev python3-opencv\n","pip install torch torchvision ultralytics\n","\n","# Exemple de script Python minimal\n","import torch\n","from PIL import Image\n","import cv2\n","import numpy as np\n","\n","# Charger modèle TorchScript\n","model = torch.jit.load(\"best.torchscript\")\n","model.eval()\n","\n","# Lire une image\n","img = cv2.imread(\"image.jpg\")\n","img_resized = cv2.resize(img, (640, 640))\n","img_tensor = torch.tensor(img_resized / 255.0).permute(2, 0, 1).unsqueeze(0).float()\n","\n","# Prédire\n","with torch.no_grad():\n","    pred = model(img_tensor)\n","```\n","\n","---\n","\n","### 📡 Utilisation avec **Arduino + ESP32/ESP8266**\n","\n","> ⚠️ YOLOv8 ne s’exécute pas directement sur un ESP32 ou un Arduino. Mais tu peux :\n","\n","* Exécuter YOLOv8 sur un **serveur (Raspberry, cloud)** et\n","* Envoyer les **résultats via HTTP/MQTT** à l’ESP32/ESP8266 pour action (LED, moteur, etc.)\n","\n","#### Exemple d’architecture :\n","\n","```text\n","Camera -> Raspberry Pi (YOLOv8) -> Envoi JSON vers ESP32 -> ESP réagit (alarme, LED, etc.)\n","```\n","\n","#### Exemple de code Python serveur :\n","\n","```python\n","import requests\n","import json\n","\n","results = model(\"photo.jpg\")\n","labels = results[0].boxes.cls.cpu().numpy().tolist()\n","\n","# Envoyer à l’ESP\n","requests.post(\"http://192.168.0.100:5000/detect\", json={\"labels\": labels})\n","```\n","\n","#### Code côté ESP (MicroPython ou Arduino C++) :\n","\n","```cpp\n","// Côté Arduino ou ESP32 - lire JSON depuis HTTP/MQTT et agir\n","if (received_label == \"plastique\") {\n","  digitalWrite(LED_PIN, HIGH);\n","}\n","```\n","\n","---\n","\n","### 📱 Déploiement mobile (Android/iOS avec TensorFlow Lite)\n","\n","* Utilise le fichier `best.tflite`\n","* Intègre-le dans une app Android via **TensorFlow Lite Android API**\n","* Exemples disponibles ici : [https://www.tensorflow.org/lite/android](https://www.tensorflow.org/lite/android)\n","\n","```java\n","// Android Java\n","Interpreter tflite = new Interpreter(loadModelFile(\"best.tflite\"));\n","```\n","\n","---\n","\n","### ✅ Récapitulatif des formats utiles\n","\n","| Format         | Utilisation                         |\n","| -------------- | ----------------------------------- |\n","| `.pt`          | PyTorch / Ultralytics               |\n","| `.torchscript` | Raspberry Pi / C++ / PyTorch Lite   |\n","| `.onnx`        | OpenCV DNN, TensorRT, NVIDIA Jetson |\n","| `.tflite`      | Android, Raspberry Pi, ESP-CAM      |\n","| `coreml`       | iOS (iPhone, iPad)                  |\n","| `saved_model`  | TensorFlow, TF.js, TPU              |\n","\n","---\n"],"metadata":{"id":"yspXNE3ux5Ma"}},{"cell_type":"markdown","source":["\n","\n","---\n","## Documentation complémentaire\n","\n","### Roboflow : https://www.google.com/url?sa=t&source=web&rct=j&opi=89978449&url=https://roboflow.com/&ved=2ahUKEwj4xKbqtOSNAxXCTaQEHZpOM5QQFnoECAkQAQ&usg=AOvVaw1h_eItJcl0qix45QGoUODw\n","\n","### Tensorflow : https://www.google.com/url?sa=t&source=web&rct=j&opi=89978449&url=https://www.tensorflow.org/%3Fhl%3Dfr&ved=2ahUKEwiz25OGteSNAxW1V6QEHQs2AyUQFnoECBUQAQ&usg=AOvVaw1C86YCtph6fCCJ-ya1LX3g\n","\n","### Pytorch : https://www.google.com/url?sa=t&source=web&rct=j&opi=89978449&url=https://pytorch.org/&ved=2ahUKEwiQzqOSteSNAxUXV6QEHTirDjMQFnoECBkQAQ&usg=AOvVaw2mABY6VbqZdRJYnleMzDSb\n","\n","### Ultralytics : https://www.google.com/url?sa=t&source=web&rct=j&opi=89978449&url=https://www.ultralytics.com/&ved=2ahUKEwih6cebteSNAxW8U6QEHSOwPP4QFnoECEEQAQ&usg=AOvVaw0egcjbygLKWOBkY23mDV-l\n"],"metadata":{"id":"gRZpgpP1zMj3"}},{"cell_type":"code","source":[],"metadata":{"id":"GmpfQIdxzTIH"},"execution_count":null,"outputs":[]}]}