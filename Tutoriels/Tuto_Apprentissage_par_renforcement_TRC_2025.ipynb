{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["#Apprentissage par Renforcement pour la Collecte de D√©chets avec ROS - TEKBOT Robotics Challenge 2025\n","\n","Dans ce notebook, nous allons apprendre √† entra√Æner un robot mobile autonome √† collecter des d√©chets dans un environnement simul√©. L'agent sera entra√Æn√© avec l'apprentissage par renforcement (RL) et pourra s‚Äôint√©grer dans un syst√®me robotique via ROS.\n","\n","Ce tutoriel s‚Äôadresse aux d√©butants/interm√©diaires en RL et ROS.\n","\n","## Objectif\n","Former un agent √† :\n","- Naviguer dans un environnement\n","- √âviter les obstacles\n","- Collecter les d√©chets\n","\n","## Technologies utilis√©es :\n","- Python\n","- OpenAI Gym (ou gymnasium)\n","- Stable-Baselines3\n","- ROS (via rosbridge ou gym-gazebo)\n","\n","## Impl√©mentation :\n","La construction de l'environnemt et l'entrainement se fera directement dans votre module ROS\n"],"metadata":{"id":"4tDy9xfQ21Tl"}},{"cell_type":"markdown","source":["# Pr√©paration de l'environnement"],"metadata":{"id":"b0Vzjyfp3K2e"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"RsHprXIF2sFV"},"outputs":[],"source":["!pip install gym stable-baselines3[extra] matplotlib numpy\n","# Optionnel ROS via Jupyter : !pip install rospkg catkin_pkg\n","import gym\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from stable_baselines3 import PPO"]},{"cell_type":"markdown","source":["## ü§ñ D√©finition de l‚Äôenvironnement personnalis√©\n","\n","Si tu ne veux pas d√©marrer avec Gazebo tout de suite, commence avec une version 2D type GridWorld :"],"metadata":{"id":"UJ4b1Ap33lfC"}},{"cell_type":"code","source":["from gym import spaces\n","from gym.utils import seeding\n","\n","class WasteCollectionEnv(gym.Env):\n","    def __init__(self, grid_size=5, n_dechets=3):\n","        self.grid_size = grid_size\n","        self.agent_pos = [0, 0]\n","        self.dechets = []\n","        self.action_space = spaces.Discrete(4)  # N, S, E, W\n","        self.observation_space = spaces.Box(low=0, high=1, shape=(grid_size, grid_size, 1), dtype=np.uint8)\n","        self._reset()\n","\n","    def _reset(self):\n","        self.agent_pos = [0, 0]\n","        self.dechets = [list(np.random.randint(0, self.grid_size, size=2)) for _ in range(3)]\n","        return self._get_obs()\n","\n","    def _get_obs(self):\n","        obs = np.zeros((self.grid_size, self.grid_size, 1), dtype=np.uint8)\n","        obs[self.agent_pos[0], self.agent_pos[1]] = 1\n","        for d in self.dechets:\n","            obs[d[0], d[1]] = 2\n","        return obs\n","\n","    def step(self, action):\n","        x, y = self.agent_pos\n","        if action == 0 and x > 0: x -= 1\n","        if action == 1 and x < self.grid_size - 1: x += 1\n","        if action == 2 and y < self.grid_size - 1: y += 1\n","        if action == 3 and y > 0: y -= 1\n","        self.agent_pos = [x, y]\n","\n","        reward = 0\n","        done = False\n","        if self.agent_pos in self.dechets:\n","            reward = 1\n","            self.dechets.remove(self.agent_pos)\n","        if len(self.dechets) == 0:\n","            done = True\n","\n","        return self._get_obs(), reward, done, {}\n","\n","    def render(self):\n","        grid = np.zeros((self.grid_size, self.grid_size))\n","        for d in self.dechets:\n","            grid[d[0], d[1]] = 0.5\n","        grid[self.agent_pos[0], self.agent_pos[1]] = 1.0\n","        print(grid)"],"metadata":{"id":"k3xpJ0GC3p79"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## ü§ñ  Entra√Ænement avec PPO"],"metadata":{"id":"dAHKH6dK3_H6"}},{"cell_type":"code","source":["from stable_baselines3 import PPO\n","from stable_baselines3.common.env_checker import check_env\n","\n","env = WasteCollectionEnv()\n","check_env(env, warn=True)\n","\n","model = PPO(\"MlpPolicy\", env, verbose=1)\n","model.learn(total_timesteps=10_000)"],"metadata":{"id":"6-V1exvv4BOk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## üìà √âvaluation"],"metadata":{"id":"Z92lSPaP4NZo"}},{"cell_type":"code","source":["obs = env.reset()\n","total_reward = 0\n","for _ in range(20):\n","    action, _ = model.predict(obs)\n","    obs, reward, done, info = env.step(action)\n","    total_reward += reward\n","    env.render()\n","    if done:\n","        break\n","print(f\"Total reward collected: {total_reward}\")"],"metadata":{"id":"Pn5tUUqJ4LZ-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Int√©gration avec ROS\n","\n","Pour passer du monde simul√© √† ROS :\n","\n","- L‚Äôenvironnement peut √™tre recr√©√© dans Gazebo\n","- L‚Äôagent peut publier des commandes (Twist) sur `/cmd_vel`\n","- Les capteurs (cam√©ra, lidar, odom) peuvent √™tre consomm√©s via ROS Bridge\n","\n","### Approches possibles :\n","\n","- Utiliser `gym-gazebo2` pour connecter RL √† ROS2\n","- Utiliser `rosbridge_suite` pour interagir via WebSocket avec Python\n","- Utiliser un n≈ìud ROS Python (`rospy`) pour connecter le mod√®le\n","\n","> Exemple de code ROS pour envoyer l‚Äôaction du mod√®le :\n","```python\n","import rospy\n","from geometry_msgs.msg import Twist\n","\n","pub = rospy.Publisher('/cmd_vel', Twist, queue_size=10)\n","rospy.init_node('rl_agent_node')\n","\n","twist = Twist()\n","twist.linear.x = 0.2  # exemple simple\n","twist.angular.z = 0.0\n","pub.publish(twist)\n","\n","    Pour connecter le mod√®le RL :\n","\n","    charger le mod√®le entra√Æn√© avec model = PPO.load('model_path')\n","\n","    transformer observation ROS ‚Üí format d‚Äôobservation RL\n","\n","    pr√©dire l‚Äôaction et publier\n","\n","Tu peux aussi utiliser ROS2 + Gazebo + gym-gazebo2 si tu veux faire une simulation plus r√©aliste.\n","\n","\n","---\n","\n","### Conclusion\n","Ce tutoriel montre comment :\n","- Cr√©er un environnement d‚Äôapprentissage par renforcement\n","- Entra√Æner un agent avec Stable-Baselines3\n","- Int√©grer ROS pour passer en environnement r√©el ou simul√©\n","\n","üéØ Prochaine √©tape :\n","- Tester dans Gazebo\n","- Ajouter des obstacles\n","- G√©rer plusieurs types de d√©chets (recyclables, non-recyclables)"],"metadata":{"id":"cNicUK8I4c4s"}},{"cell_type":"markdown","source":["\n","## Explication sur la construction de l'environnement\n","Nous d√©finissons un environnement d‚Äôapprentissage par renforcement personnalis√© pour simuler la **collecte de d√©chets dans une grille**. Il est compatible avec `gym`, une biblioth√®que utilis√©e pour entra√Æner des agents d‚Äôapprentissage par renforcement.\n","\n","---\n","\n","## üì¶ Analyse ligne par ligne\n","\n","### üß© Importations\n","\n","```python\n","from gym import spaces\n","from gym.utils import seeding\n","```\n","\n","* `spaces`: permet de d√©finir les **espaces d‚Äôactions et d‚Äôobservations**.\n","* `seeding`: pour le contr√¥le de l‚Äôal√©atoire (non utilis√© ici, mais utile pour reproductibilit√©).\n","\n","---\n","\n","### üß™ D√©finition de l‚Äôenvironnement\n","\n","```python\n","class WasteCollectionEnv(gym.Env):\n","```\n","\n","Cette classe h√©rite de `gym.Env` et doit donc impl√©menter les m√©thodes standards : `step()`, `reset()`, `render()`, etc.\n","\n","---\n","\n","### üîß Initialisation (`__init__`)\n","\n","```python\n","def __init__(self, grid_size=5, n_dechets=3):\n","```\n","\n","* `grid_size=5` ‚Üí taille de la grille carr√©e.\n","* `n_dechets=3` ‚Üí nombre de d√©chets √† collecter (non utilis√© directement ici, c‚Äôest cod√© en dur √† 3).\n","\n","```python\n","self.grid_size = grid_size\n","self.agent_pos = [0, 0]  # position de d√©part\n","self.dechets = []        # liste des positions de d√©chets\n","```\n","\n","#### üß† Espace d‚Äôaction\n","\n","```python\n","self.action_space = spaces.Discrete(4)\n","```\n","\n","* 4 actions possibles : `0 = haut`, `1 = bas`, `2 = droite`, `3 = gauche`\n","\n","#### üëÅÔ∏è Espace d‚Äôobservation\n","\n","```python\n","self.observation_space = spaces.Box(low=0, high=1, shape=(grid_size, grid_size, 1), dtype=np.uint8)\n","```\n","\n","* Observation : une **grille (image 2D)** o√π :\n","\n","  * `0` = vide\n","  * `1` = agent\n","  * `2` = d√©chet\n","\n","---\n","\n","### üßº R√©initialisation\n","\n","```python\n","def _reset(self):\n","    self.agent_pos = [0, 0]\n","    self.dechets = [list(np.random.randint(0, self.grid_size, size=2)) for _ in range(3)]\n","    return self._get_obs()\n","```\n","\n","* Replace l‚Äôagent au coin `[0, 0]`\n","* Place 3 d√©chets √† des positions al√©atoires\n","* Retourne l'observation actuelle\n","\n","---\n","\n","### üëÄ Observation\n","\n","```python\n","def _get_obs(self):\n","    obs = np.zeros((self.grid_size, self.grid_size, 1), dtype=np.uint8)\n","    obs[self.agent_pos[0], self.agent_pos[1]] = 1\n","    for d in self.dechets:\n","        obs[d[0], d[1]] = 2\n","    return obs\n","```\n","\n","* Cr√©e une **grille vide**\n","* Place l‚Äôagent et les d√©chets dans cette grille\n","\n","---\n","\n","### üîÅ √âtape d‚Äôinteraction\n","\n","```python\n","def step(self, action):\n","```\n","\n","* Met √† jour la position de l‚Äôagent en fonction de l‚Äôaction choisie :\n","\n","```python\n","if action == 0 and x > 0: x -= 1   # haut\n","if action == 1 and x < self.grid_size - 1: x += 1  # bas\n","if action == 2 and y < self.grid_size - 1: y += 1  # droite\n","if action == 3 and y > 0: y -= 1   # gauche\n","```\n","\n","* Si l‚Äôagent atteint un d√©chet :\n","\n","```python\n","if self.agent_pos in self.dechets:\n","    reward = 1\n","    self.dechets.remove(self.agent_pos)\n","```\n","\n","* Si tous les d√©chets sont ramass√©s, l‚Äô√©pisode se termine :\n","\n","```python\n","if len(self.dechets) == 0:\n","    done = True\n","```\n","\n","---\n","\n","### üñºÔ∏è Affichage console\n","\n","```python\n","def render(self):\n","    grid = np.zeros((self.grid_size, self.grid_size))\n","    for d in self.dechets:\n","        grid[d[0], d[1]] = 0.5\n","    grid[self.agent_pos[0], self.agent_pos[1]] = 1.0\n","    print(grid)\n","```\n","\n","* `0.0` ‚Üí case vide\n","* `0.5` ‚Üí d√©chet\n","* `1.0` ‚Üí agent\n","\n","---\n","\n","## ‚úÖ Exemple d‚Äôutilisation\n","\n","```python\n","env = WasteCollectionEnv()\n","obs = env._reset()\n","env.render()\n","\n","obs, reward, done, _ = env.step(2)  # Move right\n","env.render()\n","```\n","\n","---\n","\n","## üéØ Conclusion\n","\n","Ce code est un **environnement simplifi√© en grille** pour entra√Æner un agent √† **se d√©placer et ramasser des d√©chets**. Il peut facilement √™tre :\n","\n","* √âtendu √† ROS/Gazebo\n","* Connect√© √† un mod√®le PPO/DQN\n"],"metadata":{"id":"CvIp83Ad9TW8"}},{"cell_type":"markdown","source":["Utiliser **Gazebo avec ROS 2** dans un notebook pour illustrer l‚Äôapprentissage par renforcement (RL) dans la collecte de d√©chets est tr√®s pertinent et professionnalisant.\n","\n","---\n","\n","## ‚úÖ Objectif\n","\n","Construire un **notebook tutoriel** qui :\n","\n","1. Cr√©e une simulation Gazebo de robot collecteur de d√©chets (turtlebot ou mod√®le personnalis√©)\n","2. Entra√Æne un agent RL (PPO) dans un environnement `gym-gazebo2`\n","3. Connecte ROS 2 pour publier les commandes (via `ros2_control` ou `/cmd_vel`)\n","4. Fait une boucle observation ‚Üí action ‚Üí ROS\n","\n","---\n","\n","## üí° Architecture du Projet\n","\n","```\n",".\n","‚îú‚îÄ‚îÄ notebook_rl_waste_collector.ipynb  <-- Notebook tutoriel\n","‚îú‚îÄ‚îÄ gazebo_env/\n","‚îÇ   ‚îú‚îÄ‚îÄ models/                        <-- D√©chets et obstacles\n","‚îÇ   ‚îú‚îÄ‚îÄ worlds/                        <-- Monde simul√©\n","‚îÇ   ‚îî‚îÄ‚îÄ launch/\n","‚îÇ       ‚îî‚îÄ‚îÄ waste_world.launch.py     <-- Lancement de Gazebo + ROS 2\n","‚îú‚îÄ‚îÄ rl/\n","‚îÇ   ‚îî‚îÄ‚îÄ waste_collector_env.py        <-- Classe GymEnv pour ROS2\n","‚îî‚îÄ‚îÄ install.sh                        <-- Script d'installation\n","```\n","\n","---\n","\n","## üì¶ √âtapes Cl√©s du Notebook\n","\n","---\n","\n","### üß™ √âtape 1 : Setup (hors notebook)\n","\n","Avant de pouvoir tout ex√©cuter dans un notebook, il faut **pr√©parer le syst√®me** :\n","\n","### ‚úÖ Installation de ROS 2 + Gazebo + gym-gazebo2\n","\n","```bash\n","# Pour Ubuntu 22.04\n","sudo apt update\n","sudo apt install ros-humble-desktop python3-colcon-common-extensions ros-humble-gazebo-ros-pkgs\n","\n","# Cr√©er un workspace\n","mkdir -p ~/rl_ws/src\n","cd ~/rl_ws/src\n","\n","# Cloner un environnement de simulation (ex: turtlebot3 + gym-gazebo2)\n","git clone https://github.com/ros-simulation/gym-gazebo2.git\n","cd ..\n","colcon build --symlink-install\n","source install/setup.bash\n","```\n","\n","---\n","\n","### üì¶ √âtape 2 : Lancement du monde Gazebo\n","\n","```python\n","# Code dans une cellule shell de notebook Jupyter (optionnel)\n","!ros2 launch gym_gazebo2 turtlebot3_world.launch.py\n","```\n","\n","Sinon dans un terminal :\n","\n","```bash\n","ros2 launch gym_gazebo2 turtlebot3_world.launch.py\n","```\n","\n","---\n","\n","### üîÅ √âtape 3 : Cr√©ation d‚Äôun wrapper Gym pour l‚Äôenvironnement ROS 2\n","\n","```python\n","# rl/waste_collector_env.py\n","\n","import gym\n","import numpy as np\n","import rclpy\n","from rclpy.node import Node\n","from geometry_msgs.msg import Twist\n","from nav_msgs.msg import Odometry\n","\n","class WasteCollectorEnv(gym.Env):\n","    def __init__(self):\n","        super().__init__()\n","        rclpy.init()\n","        self.node = rclpy.create_node(\"rl_agent\")\n","\n","        self.pub = self.node.create_publisher(Twist, \"/cmd_vel\", 10)\n","        self.odom_sub = self.node.create_subscription(Odometry, \"/odom\", self.odom_callback, 10)\n","        self.pose = None\n","\n","        self.action_space = gym.spaces.Discrete(4)  # N, S, E, W or FWD, LEFT, RIGHT, STOP\n","        self.observation_space = gym.spaces.Box(low=0, high=1, shape=(5,), dtype=np.float32)\n","\n","    def odom_callback(self, msg):\n","        self.pose = msg.pose.pose\n","\n","    def step(self, action):\n","        twist = Twist()\n","        if action == 0:\n","            twist.linear.x = 0.3\n","        elif action == 1:\n","            twist.angular.z = 0.5\n","        elif action == 2:\n","            twist.angular.z = -0.5\n","        elif action == 3:\n","            twist.linear.x = 0.0\n","\n","        self.pub.publish(twist)\n","        rclpy.spin_once(self.node, timeout_sec=0.1)\n","\n","        obs = self._get_observation()\n","        reward = self._compute_reward()\n","        done = False  # Ajoute logique selon objectif atteint\n","        return obs, reward, done, {}\n","\n","    def _get_observation(self):\n","        if self.pose:\n","            return np.array([\n","                self.pose.position.x,\n","                self.pose.position.y,\n","                self.pose.orientation.z,\n","                self.pose.orientation.w,\n","                1.0  # Distance to nearest \"trash\" object, √† calculer\n","            ])\n","        return np.zeros(5)\n","\n","    def _compute_reward(self):\n","        # √Ä personnaliser : proximit√© avec les d√©chets\n","        return 1.0 if self.pose and self.pose.position.x > 1.0 else 0.0\n","\n","    def reset(self):\n","        # Reset via ROS service si n√©cessaire\n","        return self._get_observation()\n","```\n","\n","---\n","\n","### üß† √âtape 4 : Entra√Ænement RL dans le Notebook\n","\n","```python\n","from stable_baselines3 import PPO\n","from rl.waste_collector_env import WasteCollectorEnv\n","\n","env = WasteCollectorEnv()\n","model = PPO(\"MlpPolicy\", env, verbose=1)\n","model.learn(total_timesteps=50000)\n","model.save(\"ppo_waste_collector\")\n","```\n","\n","---\n","\n","### üìä √âtape 5 : D√©ploiement en conditions Gazebo\n","\n","```python\n","# Charger et ex√©cuter\n","model = PPO.load(\"ppo_waste_collector\")\n","\n","obs = env.reset()\n","for i in range(200):\n","    action, _states = model.predict(obs)\n","    obs, reward, done, info = env.step(action)\n","    if done:\n","        break\n","```\n","\n","---\n","\n","## üîå Bonus : Int√©gration ROS 2 compl√®te\n","\n","Si tu veux aller plus loin :\n","\n","* Cr√©e un n≈ìud ROS 2 `rl_agent_node.py` qui importe le mod√®le entra√Æn√©\n","* Publie directement sur `/cmd_vel` √† chaque pr√©diction\n","* Abonne-toi √† `/odom` et √©ventuellement `/camera` pour extraire les observations\n","\n","---\n","\n","## üìÅ Fichier `waste_world.world` (exemple simple Gazebo)\n","\n","```xml\n","<world name=\"waste_world\">\n","  <include>\n","    <uri>model://ground_plane</uri>\n","  </include>\n","  <include>\n","    <uri>model://turtlebot3_burger</uri>\n","  </include>\n","  <include>\n","    <uri>model://trash_can</uri>\n","    <pose>1 2 0 0 0 0</pose>\n","  </include>\n","</world>\n","```\n","\n","---\n"],"metadata":{"id":"_9ys7ful_Lt1"}},{"cell_type":"markdown","source":["Lorsqu'on **entra√Æne un agent avec l'apprentissage par renforcement (RL)**, l‚Äôobjectif final est souvent d‚Äô**exporter un mod√®le entra√Æn√©** afin de l‚Äôutiliser dans un syst√®me r√©el comme un robot (via **ROS2 + Gazebo**, par exemple).\n","\n","---\n","\n","## üîÅ Pipeline g√©n√©ral d‚Äôun agent RL\n","\n","1. üì¶ **D√©finir l‚Äôenvironnement (`env`)** ‚Äî comme celui que tu as cod√©.\n","2. üß† **Entra√Æner un agent** (ex. avec `Stable-Baselines3`, `RLlib`, etc.)\n","3. üíæ **Sauvegarder le mod√®le**\n","4. üöÄ **Charger le mod√®le pour l‚Äôutiliser en production ou dans un simulateur comme Gazebo**\n","\n","---\n","\n","## üß† Exemple avec `Stable-Baselines3`\n","\n","### 1. Entra√Ænement de l‚Äôagent\n","\n","```python\n","from stable_baselines3 import PPO\n","from stable_baselines3.common.env_checker import check_env\n","\n","env = WasteCollectionEnv()\n","check_env(env)  # v√©rifie que l‚Äôenvironnement est compatible\n","\n","model = PPO(\"MlpPolicy\", env, verbose=1)\n","model.learn(total_timesteps=10000)\n","```\n","\n","---\n","\n","### 2. Sauvegarde du mod√®le\n","\n","```python\n","model.save(\"ppo_waste_collector\")\n","```\n","\n","Cela va cr√©er un dossier ou fichier contenant :\n","\n","* Les **poids du mod√®le**\n","* La **structure du r√©seau de neurones**\n","* Les **hyperparam√®tres**\n","\n","---\n","\n","### 3. Chargement du mod√®le pour utilisation\n","\n","```python\n","from stable_baselines3 import PPO\n","\n","model = PPO.load(\"ppo_waste_collector\")\n","\n","obs = env.reset()\n","done = False\n","while not done:\n","    action, _states = model.predict(obs)\n","    obs, reward, done, info = env.step(action)\n","    env.render()\n","```\n","\n","---\n","\n","## üß© Int√©gration dans ROS2 + Gazebo\n","\n","Une fois le mod√®le entra√Æn√© :\n","\n","### üîÅ Option 1 ‚Äì Utiliser en Python dans ROS2\n","\n","Tu peux cr√©er un **n≈ìud ROS2 en Python** qui :\n","\n","* Charge le mod√®le\n","* R√©cup√®re l‚Äô√©tat du robot via les topics `/odom`, `/scan`, etc.\n","* Produit une action avec `model.predict(obs)`\n","* Envoie une commande sur `/cmd_vel` ou autre\n","\n","```python\n","import rclpy\n","from rclpy.node import Node\n","from geometry_msgs.msg import Twist\n","from stable_baselines3 import PPO\n","import numpy as np\n","\n","class RLController(Node):\n","    def __init__(self):\n","        super().__init__('rl_controller')\n","        self.model = PPO.load(\"ppo_waste_collector\")\n","        self.publisher = self.create_publisher(Twist, '/cmd_vel', 10)\n","        self.timer = self.create_timer(0.1, self.control_loop)\n","\n","    def control_loop(self):\n","        obs = self.get_observation_from_sensors()\n","        action, _ = self.model.predict(obs)\n","        cmd = self.convert_action_to_cmd_vel(action)\n","        self.publisher.publish(cmd)\n","```\n","\n","---\n","\n","### üìÅ Option 2 ‚Äì Exporter vers ONNX ou TorchScript\n","\n","Tu peux exporter ton mod√®le pour l‚Äôint√©grer dans un autre langage ou d√©ployer sur des syst√®mes embarqu√©s :\n","\n","#### üî∑ Export vers ONNX\n","\n","```python\n","import torch\n","\n","model.policy.to(\"cpu\")\n","dummy_input = torch.randn(1, model.observation_space.shape[0])\n","torch.onnx.export(model.policy, dummy_input, \"waste_policy.onnx\")\n","```\n","\n","Ce fichier `.onnx` peut ensuite √™tre utilis√© avec :\n","\n","* **ROS2 + ONNX Runtime**\n","* **NVIDIA Jetson (TensorRT)**\n","* **Unity / Web / Mobile**\n","\n","---\n","\n","## üöÄ R√©sum√©\n","\n","| √âtape        | Objectif               | Code cl√©                     |\n","| ------------ | ---------------------- | ---------------------------- |\n","| Entra√Ænement | Apprendre la strat√©gie | `model.learn(...)`           |\n","| Sauvegarde   | Stocker le mod√®le      | `model.save(\"...\")`          |\n","| Chargement   | R√©utiliser             | `PPO.load(\"...\")`            |\n","| D√©ploiement  | ROS2, Gazebo, Jetson   | `model.predict(obs)` ou ONNX |\n","\n","---\n","\n","Souhaites-tu un **exemple complet de n≈ìud ROS2** qui charge le mod√®le entra√Æn√© et contr√¥le un robot dans Gazebo ?\n"],"metadata":{"id":"dqNWh9HQIrxh"}}]}